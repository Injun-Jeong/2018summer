perceptron (layer)이란?
- 뉴런을 모방한 구조
- 여러 입력들이 weight만큼 곱해지고 이 값을 모두 합친 것과  bias의 합이 activation function (ex. sigmoid, ...)을 통괗여 최종 출력으로 나가게 되는 구조



layer의 종류도 다양한데 나중에 알아보자.
cf. 3가지 종류
- 합성곱 계층 (convolution layer)
- 통합 계층 (pooling layer)
- 완전 연결 계층 (fully connected layer)



layer를 많이 사용하면 좋기 때문에 다중 layer를 사용한다?
- 아니다. 다중 layer를 사용하는 이유는 단일 layer보다 무조건 적으로 좋은 것이 아니라, 단일 layer만으로 해결이 불가능한 (ex. XOR problem, ...) 것을 다중 layer를 사용하면 해결 할 수 있다.
- 위와 같은 이유로 perceptron을 여러층 쌓은 MLP (Multi Layer Perceptron)의 '필요성'을 주장



Backpropagation: 오차를 출력층에서 입력층까지 거슬러 올리는 것



Hidden layer가 너무 많은 경우 오차가 전달되면서 0으로 소멸하여 사라지거나, 또는 무한대로 발산하는 문제가 발생
- 이러한 문제를 vanish gradient 또는 exploding gradient라고 부름
- 위와 같은 문제를 해결 autoencoder -> 많은 layer 사용 가능













