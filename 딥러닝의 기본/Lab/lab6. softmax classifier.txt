Softmax function
[code]
hypothesis = tf.nn.softmax(tf.matmul(X, W) +b)



Cost function: cross entropy
[code]
cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis), axis=1)



Test & one-hot encoding
[example code]
a = sess.run(hypothesis, feed_dict = {X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})
print(sess.run(tf.arg_max(a, 1)))	(m.t.) (a, 1)의 1은 1차원 array로 return 하라는 의미인 듯

[output]
[1 0 2]		// means that [1, 11, 7, 9] is class 1, [1, 3, 4, 3] is class 0, and [1, 1, 0, 1] is class 2.



softmax_cross_entropy_with_logits
[code]
logits = tf.matmul(X, W) + b
hypothesis = tf.nn.softmax(logits)

cost를 표현하는 2가지 방법
# 방법 1
cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis), axis=1))

# 방법 2 by using the function of softmax_cross_entropy_with_logits
cost_1 = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_one_hot)
cost = tf.reduce_mean(cost_i)












