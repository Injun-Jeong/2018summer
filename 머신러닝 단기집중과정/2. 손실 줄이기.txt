//회귀에 사용할 때 편리한 손실 함수
- 제곱 오차라고도 함
- 예측과 라벨간의 차이 제곱 = (y - y')^2

//머신러닝의 관습에 따른 모델의 방정식
	y' = b + w_1 * x_1
- y'는 예측된 라벨
- b는 편향(y절편). 일부 머신러닝 자료에서는 w_0이라고도 함
- w_1은 특성 1의 가중치
  - 가중치: 선형 모델에서 특성의 계수
- x_1은 특성

//모델을 학습시킨다는 것은 단순히 말하자면 라벨이 있는 데이터로부터 올바른 가중치와 편향값을 학습(결정)하는 것

//머신러닝 알고리즘은 다양한 예를 검토하고 손실을 최소화 하는 모델을 찾아봄으로써 모델을 만들어내는데, 이 과정을 경험적 위험 최소화

//손실은 잘못된 예측에 대한 벌점
- 즉, 손실은 한 가지 예에서 모델의 예측이 얼마나 잘못되었는지를 나타내는 수
- 모델의 예측이 완벽하면 손실은 0이고 그렇지 않으면 손실은 그보다 커짐
- 손실 >= 0

//평균 제곱 오차(MSE)는 '예시당' 평균 제곱 손실
- MSE는 머신러닝에서 흔히 사용되지만, 모든 상황에서 최선인 유일한 손실 함수는 아님
- RMSE(Root Mean Square Error / 평균 제곱근 오차): '평균 제곱 오차'에 root를 취한 것

//손실 줄이기
- 모델을 학습하려면 모델의 손실을 줄이기 위한 좋은 방법이 필요
- 반복 방식은 손실을 줄이는 데 사용되는 일반적인 방법 중 하나로 매우 간편하고 효율적
- x축을 기울기, y축을 

//학습률이 높고 낮음에 따라 손실 곡선의 최저점에 도달하는 단계 수를 더 작거나, 혹은 영원히 도달하지 못할 수 있음
- 따라서, 최저점에 도달하는 단계의 수를 최소화하는 골디락스(최적의) 학습률 필요
  - 골디락스: 일반적으로 너무 뜨겁지도 않은, 너무 차갑지도 않은, 딱 적당한 상태를 가리킴
  - 즉, 최적의 상태
- 실무에서는 모델 학습의 성공을 위해 최적 또는 최적에 근접한 학습률을 반드시 구할 필요는 없음
- 경사하강법이 효과적으로 수렴할 정도로 크지만 발산할 정도로 크지는 않은 적당한 학습률을 구하는 것이 목표

//높은 차원(데이터의 요소가 많은)의 모델에서는 손실 곡선이 단계가 커짐에 따라 발산할 가능성 큼
- 이러한 상황에서는 학습률을 낮출 필요

//손실 곡선의 한 지점에서 출발해 극솟값으로 닿길 바람
- 이때, 출발점이 중요?
- 손실 곡선이 단순한 이차곡선(선형 회귀 문제)의 경우 중요하지 않을 수 있지만, 머신 러닝 문제는 이차곡선과 같이 볼록하지 않은 경우가 많음
- 즉, 극솟값을 찾되 최솟값은 아닐 수 있음
- 따라서, 초깃값(출발하는 지점)이 중요

//효율성
- 손실 함수에서 경사를 계산할 때, 수학적인 방식을 사용하면 데이터 세트 안에 있는 모든 예시의 경사를 구해야 함
- 이것은 비효율적
- 전체 데이터 세트가 아니라, 하나의 예시(벡터?)에서만 손실의 경사를 계산해도 괜찮음이 경험적으로 입증됨
- 위와 같은 방법을 사용하면 더 큰 보폭으로 이동해야 하지만, 해결책에 도달하는데 드는 전체 계산량은 훨씬 적어짐
- 이러한 방법을 '확률적 경사하강법(SGD)'
- 실무에서는 이 둘의 중간쯤에 있는 방법을 사용
- 하나의 예시 또는 전체 데이터 세트가 아닌, 10개에서 1,000개 정도의 예시가 포함된 배치를 사용
- 이러한 방법을 '미니 배치 경사하강법'

//반복 방식

	요소들 -> 모델(예측 함수) -> 추론(예측) -> 손실 계산 -> (손실 함수의 값을 검토하여) 매개변수 업데이트 계산 -> 모델(예측 함수) -> 추론(예측) -> 손실 계산 -> 매개변수 업데이트 계산 -> 모델(예측 함수) -> ...
	라벨 -> 손실 계산 -> 매개변수 업데이트 계산 -> 모델(예측 함수) -> 손실 계산 -> 매개변수 업데이트 계산 -> 모델(예측 함수) -> ...

- 위는 반복방식의 모델 학습
- 반복 전략은 주로 대규모 데이터 세트에 적용하기 용이하여 머신러닝에서 널리 사용
- 보통 전체 손실이 변하지 않거나 매우 느리게 변할 때까지 계속 반복
- 이때 모델이 수렴했다고 말함
- 요점: 머신러닝 모델은 가중치와 편향에 대한 초기 예상 값에서 시작하고 예상 손실 값이 가장 적은 가중치와 편향을 학습할 때까지 이러한 예상 값을 조정

//경사하강법
- 전체 데이터 세트에 대해 상상할 수 있는 모든 가중치들의 손실 함수를 계산하는 것은 수렴 지점을 찾는 데 비효율적인 방법
- 따라서 머신러닝에서 널리 사용하는 더 나은 방법인 경사하강법
- 시작점에서 손실 곡선의 기울기를 계산
- 위에서 계산된 기울기 값은 편미분의 벡터로서, 어느 방향이 '더 정확한지' 혹은 '더 부정확한지' 알려줌
- 기울기 계산은 텐서플로우에서 전부 처리함
- 기울기는 벡터이므로 다음 두 가지 특징을 모두 가짐
  - 방향
  - 크기
- 기울기는 항상 손실 함수 값이 가장 크게 증가하는 방향을 향함
- 경사하강법 알고리즘은 가능한 한 빨리 손실을 줄이기 위해 기울기의 반대 방향으로 이동
- 기울기 보폭(학습률)을 통해 손실 곡선의 다음 지점으로 이동
- 이러한 과정을 반복하여 최소값에 점점 접근

//학습률
- 경사하강법 알고리즘은 기울기에 학습률 또는 보폭이라 불리는 스칼라를 곱하여 다음 지점을 결정
- 예를 들어, 기울기가 2.5이고 학습률이 0.01이면 경사하강법 알고리즘은 이전 지점으로 부터 0.025 떨어진 지점을 다음 지점으로 결정
- 초매개변수: 프로그래머가 머신러닝 알고리즘에서 조정하는 값
  - 대부분의 머신러닝 프로그래머는 학습률을 미세 조정하는 데 상당한 시간을 소비
  - 학습률을 너무 작게 설정하면 학습 시간이 매우 오래 걸릴 것
  - 반대로 학습률을 너무 크게 설정하면 다음 지점이 곡선의 최저점을 무질서하게 이탈할 우려
- 모든 회귀 문제에는 골디락스(최적의) 학습률이 있음
  - 1차원의 경우 이상적인 학습률은 1/f(x)''
  - 2차원 이상에서 이상적인 학습률은 '헤시안 행렬의 역' >>> 헤세 행렬 참고

//확률적 경사하강법
- 경사하강법에서 배치는 단일 반복에서 기울기를 계산하는 데 사용하는 예의 총 개수
  - 배치: 모델 학습의 반복 1회, 즉 경사 업데이트 1회에 사용되는 예의 집합
-  배치가 너무 커지면 단일 반복으로도 계산하는 데 오랜 시간이 걸릴 수 있음
- 무작위로 샘플링된 예가 포함된 대량의 데이터 세트에는 중복 데이터가 포함되어 있을 수 있음
- 실제로 배치 크기가 커지면 중복의 가능성도 그만큼 높아짐
  - 배치 크기:  배치 하나에 포함되는 예의 개수
- 적당한 중복성은 노이즈가 있는 기울기를 평활화하는 데 유용할 수 있지만, 배치가 거대해지면 예측성이 훨씬 높은 값이 대용량 배치에 비해 덜 포함되는 경향
- 확률적 경사하강법(SGD)은 반복당 하나의 예(배치 크기 1)만을 사용
- 반복이 충분하면 SGD가 효과는 있지만 노이즈가 매우 심함
- 미니 배치 확률적 경사하강법(미니 배치 SGD)는 전체 배치 반복과 SGD 간의 절충안
- 미니 배치는 일반적으로 무작위로 선택한 10개에서 1,000개 사이의 예로 구성
- 미니 배치 SGD는 SGD의 노이즈를 줄이면서도 전체 배치보다는 더 효율적







